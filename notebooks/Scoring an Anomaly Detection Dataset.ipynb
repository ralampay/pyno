{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeae217",
   "metadata": {},
   "source": [
    "# Scoring an Anomaly Detection Dataset\n",
    "\n",
    "This is a proposal to provide a score for an anomaly detection dataset to measure its difficulty when attempting to perform anomaly detection methods against it. This is largely based on the paper https://arxiv.org/pdf/1503.01158v2.pdf. \n",
    "\n",
    "We focus on two major properties namely:\n",
    "1. Relative Frequency / Ratio of anomalies against data points in a dataset\n",
    "2. Semantic variation  / clusterdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9f3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and path relative to project\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff749a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset '../data/creditcardfraud.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Reading chunk 22\n",
      "Reading chunk 23\n",
      "Reading chunk 24\n",
      "Reading chunk 25\n",
      "Reading chunk 26\n",
      "Reading chunk 27\n",
      "Reading chunk 28\n",
      "Reading chunk 29\n",
      "Done loading dataset...\n",
      "Input Dimensionality: 29\n"
     ]
    }
   ],
   "source": [
    "# Setup the dataset\n",
    "\n",
    "# Instantiate pandas DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Chunk size for reading data\n",
    "chunksize = 10000\n",
    "\n",
    "# The reference to the dataset. Change this to \n",
    "dataset_file = '../data/creditcardfraud.csv'\n",
    "\n",
    "print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "# Read each chunk and append to data frame\n",
    "for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "    print(\"Reading chunk %d\" % (i + 1))\n",
    "    data = data.append(chunk)\n",
    "\n",
    "print(\"Done loading dataset...\")\n",
    "    \n",
    "# Check for proper value of input dimensionality to be used by model\n",
    "input_dim = len(data.columns) - 1\n",
    "print(\"Input Dimensionality: %d\" % (input_dim))\n",
    "\n",
    "# Partition the data into positive_data and negative_data\n",
    "positive_data = data[data[input_dim] == 1].iloc[:,:input_dim]\n",
    "negative_data = data[data[input_dim] == -1].iloc[:,:input_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842112d",
   "metadata": {},
   "source": [
    "## Relative Frequency\n",
    "\n",
    "This simply the ratio of number of anomalies in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13e5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Frequency: 0.17275\n"
     ]
    }
   ],
   "source": [
    "# Divide the number of anomalies vs the entire length of the dataset\n",
    "# X: A pandas data frame\n",
    "def score_relative_frequency(X):\n",
    "    # Class column is always the last value\n",
    "    idx_class = len(X.columns) - 1\n",
    "    anomalies = X[X[idx_class] == -1]\n",
    "    \n",
    "    # Return the score in percentage format\n",
    "    return (len(anomalies) / len(X)) * 100\n",
    "\n",
    "print(\"Relative Frequency: %0.5f\" % (score_relative_frequency(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9491",
   "metadata": {},
   "source": [
    "## Semantic Variation\n",
    "\n",
    "A normalized clusterdness measure of given the following equation:\n",
    "\n",
    "$$\\log(\\frac{\\sigma^2_{n}}{\\sigma^2_{a}})$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\sigma^2_{n}$ is the variance of normal data\n",
    "* $\\sigma^2_{a}$ is the variance of anomaly data\n",
    "\n",
    "To deal with multi-dimensional data, we compute for the $\\sigma^2$ by taking the covariance matrix of the data $X$ using the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(X) = \\mathbf{E}[(X - \\mathbf{E}(X))(X - \\mathbf{E}(X))^{T}]\n",
    "\\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{Var}(X_1) & \\cdots &\n",
    "\\mathbf{Cov}(X_1, X_p) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{Cov}{X_p, X_1} & \\cdots &\n",
    "\\mathbf{Var}(X_p)\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "=\n",
    "\\frac{1}{n - 1}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})^2 & \\cdots &\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})(X_{ip} - \\hat{X}_{p}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})(X_{i1} - \\hat{X}_{1})  & \\cdots &\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then take trace of the covariance matrix to give us the overall variance:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\operatorname{tr}({\\mathbf{Var}(X)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3aeb1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic variation: -1.81614\n"
     ]
    }
   ],
   "source": [
    "def score_semantic_variation(X):\n",
    "    idx_class = len(X.columns) - 1\n",
    "    \n",
    "    # Partition the data into positive_data and negative_data\n",
    "    positive_data = X[X[idx_class] == 1].iloc[:,:idx_class]\n",
    "    negative_data = X[X[idx_class] == -1].iloc[:,:idx_class]\n",
    "    \n",
    "    var_n = np.trace(positive_data.cov().values)\n",
    "    var_a = np.trace(negative_data.cov().values)\n",
    "    \n",
    "    return np.log(var_n / var_a)\n",
    "    \n",
    "print(\"Semantic variation: %0.5f\" % (score_semantic_variation(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2ff82",
   "metadata": {},
   "source": [
    "## Test against public datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a2b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/annthyroid.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset annthytoid...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/backdoor.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Done loading dataset backdoor...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/bald.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Done loading dataset bald...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/bank.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Done loading dataset bank...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/cover.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Reading chunk 22\n",
      "Reading chunk 23\n",
      "Reading chunk 24\n",
      "Reading chunk 25\n",
      "Reading chunk 26\n",
      "Reading chunk 27\n",
      "Reading chunk 28\n",
      "Reading chunk 29\n",
      "Done loading dataset cover...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/creditcardfraud.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Reading chunk 22\n",
      "Reading chunk 23\n",
      "Reading chunk 24\n",
      "Reading chunk 25\n",
      "Reading chunk 26\n",
      "Reading chunk 27\n",
      "Reading chunk 28\n",
      "Reading chunk 29\n",
      "Done loading dataset creditcardfraud...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/donors.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Reading chunk 22\n",
      "Reading chunk 23\n",
      "Reading chunk 24\n",
      "Reading chunk 25\n",
      "Reading chunk 26\n",
      "Reading chunk 27\n",
      "Reading chunk 28\n",
      "Reading chunk 29\n",
      "Reading chunk 30\n",
      "Reading chunk 31\n",
      "Reading chunk 32\n",
      "Reading chunk 33\n",
      "Reading chunk 34\n",
      "Reading chunk 35\n",
      "Reading chunk 36\n",
      "Reading chunk 37\n",
      "Reading chunk 38\n",
      "Reading chunk 39\n",
      "Reading chunk 40\n",
      "Reading chunk 41\n",
      "Reading chunk 42\n",
      "Reading chunk 43\n",
      "Reading chunk 44\n",
      "Reading chunk 45\n",
      "Reading chunk 46\n",
      "Reading chunk 47\n",
      "Reading chunk 48\n",
      "Reading chunk 49\n",
      "Reading chunk 50\n",
      "Reading chunk 51\n",
      "Reading chunk 52\n",
      "Reading chunk 53\n",
      "Reading chunk 54\n",
      "Reading chunk 55\n",
      "Reading chunk 56\n",
      "Reading chunk 57\n",
      "Reading chunk 58\n",
      "Reading chunk 59\n",
      "Reading chunk 60\n",
      "Reading chunk 61\n",
      "Reading chunk 62\n",
      "Done loading dataset donors...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/kddcup99.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Done loading dataset kddcup99...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/magic04.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Done loading dataset magic04...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/mammography.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Done loading dataset mammography...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/musk.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset musk...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/pageblocks.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset pageblocks...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/seismic.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset seismic...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/shuttle.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Done loading dataset shuttle...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/speech.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset speech...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/synthetic.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset synthetic...\n",
      "Loading dataset 'https://happy-research.s3.ap-southeast-1.amazonaws.com/waveform.csv'...\n",
      "Reading chunk 1\n",
      "Done loading dataset waveform...\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    { \"name\": \"annthyroid\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/annthyroid.csv\" },\n",
    "    { \"name\": \"backdoor\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/backdoor.csv\" },\n",
    "    { \"name\": \"bald\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/bald.csv\" },\n",
    "    { \"name\": \"bank\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/bank.csv\" },\n",
    "    { \"name\": \"cover\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/cover.csv\" },\n",
    "    { \"name\": \"creditcardfraud\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/creditcardfraud.csv\" },\n",
    "    { \"name\": \"donors\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/donors.csv\" },\n",
    "    { \"name\": \"kddcup99\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/kddcup99.csv\" },\n",
    "    { \"name\": \"magic04\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/magic04.csv\" },\n",
    "    { \"name\": \"mammography\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/mammography.csv\" },\n",
    "    { \"name\": \"musk\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/musk.csv\" },\n",
    "    { \"name\": \"pageblocks\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/pageblocks.csv\" },\n",
    "    { \"name\": \"seismic\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/seismic.csv\" },\n",
    "    { \"name\": \"shuttle\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/shuttle.csv\" },\n",
    "    { \"name\": \"speech\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/speech.csv\" },\n",
    "    { \"name\": \"synthetic\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/synthetic.csv\" },\n",
    "    { \"name\": \"waveform\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/waveform.csv\" }\n",
    "]\n",
    "\n",
    "scores = [[\"Dataset\", \"Relative Frequency\", \"Semantic Variation\"]]\n",
    "\n",
    "for o in datasets:\n",
    "    # Instantiate pandas DataFrame\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Chunk size for reading data\n",
    "    chunksize = 10000\n",
    "\n",
    "    # The reference to the dataset. Change this to \n",
    "    dataset_file = o[\"location\"]\n",
    "\n",
    "    print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "    # Read each chunk and append to data frame\n",
    "    for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "        print(\"Reading chunk %d\" % (i + 1))\n",
    "        data = data.append(chunk)\n",
    "\n",
    "    print(\"Done loading dataset %s...\" % (o[\"name\"]))\n",
    "    \n",
    "    score_rf = score_relative_frequency(data)\n",
    "    score_sv = score_semantic_variation(data)\n",
    "    \n",
    "    scores.append([\n",
    "        o[\"name\"],\n",
    "        score_rf,\n",
    "        score_sv\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a39340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Dataset        </td><td>Relative Frequency</td><td>Semantic Variation  </td></tr>\n",
       "<tr><td>annthytoid     </td><td>7.416666666666667 </td><td>0.204576148176165   </td></tr>\n",
       "<tr><td>backdoor       </td><td>2.443118043827167 </td><td>0.3820243613622607  </td></tr>\n",
       "<tr><td>bald           </td><td>2.244334868385333 </td><td>0.20706683197942483 </td></tr>\n",
       "<tr><td>bank           </td><td>11.265417111780131</td><td>-0.0702057753541661 </td></tr>\n",
       "<tr><td>cover          </td><td>0.9603283365029646</td><td>0.5323430239198219  </td></tr>\n",
       "<tr><td>creditcardfraud</td><td>0.1727485630620034</td><td>-1.816141037269964  </td></tr>\n",
       "<tr><td>donors         </td><td>5.927411411760527 </td><td>1.2317944589464498  </td></tr>\n",
       "<tr><td>kddcup99       </td><td>0.4074081772641294</td><td>-1.5189374087035048 </td></tr>\n",
       "<tr><td>magic04        </td><td>35.16298633017876 </td><td>-0.46289559058591584</td></tr>\n",
       "<tr><td>mammography    </td><td>2.3249575248144505</td><td>0.4204377120843123  </td></tr>\n",
       "<tr><td>musk           </td><td>15.413761745983631</td><td>0.4053547930815733  </td></tr>\n",
       "<tr><td>pageblocks     </td><td>9.37268002969562  </td><td>-1.4199687433991703 </td></tr>\n",
       "<tr><td>seismic        </td><td>6.581494386372436 </td><td>0.06912255916932594 </td></tr>\n",
       "<tr><td>shuttle        </td><td>7.094381152301954 </td><td>-0.6028400475354985 </td></tr>\n",
       "<tr><td>speech         </td><td>1.655359565807327 </td><td>0.05144099107654203 </td></tr>\n",
       "<tr><td>synthetic      </td><td>16.666666666666664</td><td>0.013906628073977485</td></tr>\n",
       "<tr><td>waveform       </td><td>2.8762347472399767</td><td>0.11320312721500096 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>Dataset        </td><td>Relative Frequency</td><td>Semantic Variation  </td></tr>\\n<tr><td>annthytoid     </td><td>7.416666666666667 </td><td>0.204576148176165   </td></tr>\\n<tr><td>backdoor       </td><td>2.443118043827167 </td><td>0.3820243613622607  </td></tr>\\n<tr><td>bald           </td><td>2.244334868385333 </td><td>0.20706683197942483 </td></tr>\\n<tr><td>bank           </td><td>11.265417111780131</td><td>-0.0702057753541661 </td></tr>\\n<tr><td>cover          </td><td>0.9603283365029646</td><td>0.5323430239198219  </td></tr>\\n<tr><td>creditcardfraud</td><td>0.1727485630620034</td><td>-1.816141037269964  </td></tr>\\n<tr><td>donors         </td><td>5.927411411760527 </td><td>1.2317944589464498  </td></tr>\\n<tr><td>kddcup99       </td><td>0.4074081772641294</td><td>-1.5189374087035048 </td></tr>\\n<tr><td>magic04        </td><td>35.16298633017876 </td><td>-0.46289559058591584</td></tr>\\n<tr><td>mammography    </td><td>2.3249575248144505</td><td>0.4204377120843123  </td></tr>\\n<tr><td>musk           </td><td>15.413761745983631</td><td>0.4053547930815733  </td></tr>\\n<tr><td>pageblocks     </td><td>9.37268002969562  </td><td>-1.4199687433991703 </td></tr>\\n<tr><td>seismic        </td><td>6.581494386372436 </td><td>0.06912255916932594 </td></tr>\\n<tr><td>shuttle        </td><td>7.094381152301954 </td><td>-0.6028400475354985 </td></tr>\\n<tr><td>speech         </td><td>1.655359565807327 </td><td>0.05144099107654203 </td></tr>\\n<tr><td>synthetic      </td><td>16.666666666666664</td><td>0.013906628073977485</td></tr>\\n<tr><td>waveform       </td><td>2.8762347472399767</td><td>0.11320312721500096 </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display result in tabular format\n",
    "tabulate.tabulate(scores, tablefmt='html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
