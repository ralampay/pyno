{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffeae217",
   "metadata": {},
   "source": [
    "# Scoring an Anomaly Detection Dataset\n",
    "\n",
    "This is a proposal to provide a score for an anomaly detection dataset to measure its difficulty when attempting to perform anomaly detection methods against it. This is largely based on the paper https://arxiv.org/pdf/1503.01158v2.pdf. \n",
    "\n",
    "We focus on two major properties namely:\n",
    "1. Relative Frequency / Ratio of anomalies against data points in a dataset\n",
    "2. Semantic variation  / clusterdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and path relative to project\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff749a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the dataset\n",
    "\n",
    "# Instantiate pandas DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Chunk size for reading data\n",
    "chunksize = 10000\n",
    "\n",
    "# The reference to the dataset. Change this to \n",
    "dataset_file = '../data/creditcardfraud.csv'\n",
    "\n",
    "print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "# Read each chunk and append to data frame\n",
    "for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "    print(\"Reading chunk %d\" % (i + 1))\n",
    "    data = data.append(chunk)\n",
    "\n",
    "print(\"Done loading dataset...\")\n",
    "    \n",
    "# Check for proper value of input dimensionality to be used by model\n",
    "input_dim = len(data.columns) - 1\n",
    "print(\"Input Dimensionality: %d\" % (input_dim))\n",
    "\n",
    "# Partition the data into positive_data and negative_data\n",
    "positive_data = data[data[input_dim] == 1].iloc[:,:input_dim]\n",
    "negative_data = data[data[input_dim] == -1].iloc[:,:input_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842112d",
   "metadata": {},
   "source": [
    "## Relative Frequency\n",
    "\n",
    "This simply the ratio of number of anomalies in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the number of anomalies vs the entire length of the dataset\n",
    "# X: A pandas data frame\n",
    "def score_relative_frequency(X):\n",
    "    # Class column is always the last value\n",
    "    idx_class = len(X.columns) - 1\n",
    "    anomalies = X[X[idx_class] == -1]\n",
    "    \n",
    "    # Return the score in percentage format\n",
    "    return (len(anomalies) / len(X)) * 100\n",
    "\n",
    "print(\"Relative Frequency: %0.5f\" % (score_relative_frequency(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f9491",
   "metadata": {},
   "source": [
    "## Semantic Variation\n",
    "\n",
    "A normalized clusterdness measure of given the following equation:\n",
    "\n",
    "$$\\log(\\frac{\\sigma^2_{n}}{\\sigma^2_{a}})$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\sigma^2_{n}$ is the variance of normal data\n",
    "* $\\sigma^2_{a}$ is the variance of anomaly data\n",
    "\n",
    "To deal with multi-dimensional data, we compute for the $\\sigma^2$ by taking the covariance matrix of the data $X$ using the equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(X) = \\mathbf{E}[(X - \\mathbf{E}(X))(X - \\mathbf{E}(X))^{T}]\n",
    "\\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{Var}(X_1) & \\cdots &\n",
    "\\mathbf{Cov}(X_1, X_p) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{Cov}{X_p, X_1} & \\cdots &\n",
    "\\mathbf{Var}(X_p)\n",
    "\\end{bmatrix} \n",
    "\\\\\n",
    "=\n",
    "\\frac{1}{n - 1}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})^2 & \\cdots &\n",
    "\\sum_{i=1}^n(X_{i1} - \\hat{X}_{1})(X_{ip} - \\hat{X}_{p}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})(X_{i1} - \\hat{X}_{1})  & \\cdots &\n",
    "\\sum_{i=1}^n(X_{ip} - \\hat{X}_{p})^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then take trace of the covariance matrix to give us the overall variance:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\operatorname{tr}({\\mathbf{Var}(X)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_semantic_variation(X):\n",
    "    idx_class = len(X.columns) - 1\n",
    "    \n",
    "    # Partition the data into positive_data and negative_data\n",
    "    positive_data = X[X[idx_class] == 1].iloc[:,:idx_class]\n",
    "    negative_data = X[X[idx_class] == -1].iloc[:,:idx_class]\n",
    "    \n",
    "    var_n = np.trace(positive_data.cov().values)\n",
    "    var_a = np.trace(negative_data.cov().values)\n",
    "    \n",
    "    return np.log(var_n / var_a)\n",
    "    \n",
    "print(\"Semantic variation: %0.5f\" % (score_semantic_variation(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2ff82",
   "metadata": {},
   "source": [
    "## Test against public datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    { \"name\": \"annthyroid\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/annthyroid.csv\" },\n",
    "    { \"name\": \"backdoor\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/backdoor.csv\" },\n",
    "    { \"name\": \"bald\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/bald.csv\" },\n",
    "    { \"name\": \"bank\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/bank.csv\" },\n",
    "    { \"name\": \"cover\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/cover.csv\" },\n",
    "    { \"name\": \"creditcardfraud\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/creditcardfraud.csv\" },\n",
    "    { \"name\": \"donors\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/donors.csv\" },\n",
    "    { \"name\": \"kddcup99\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/kddcup99.csv\" },\n",
    "    { \"name\": \"magic04\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/magic04.csv\" },\n",
    "    { \"name\": \"mammography\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/mammography.csv\" },\n",
    "    { \"name\": \"musk\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/musk.csv\" },\n",
    "    { \"name\": \"pageblocks\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/pageblocks.csv\" },\n",
    "    { \"name\": \"seismic\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/seismic.csv\" },\n",
    "    { \"name\": \"shuttle\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/shuttle.csv\" },\n",
    "    { \"name\": \"speech\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/speech.csv\" },\n",
    "    { \"name\": \"synthetic\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/synthetic.csv\" },\n",
    "    { \"name\": \"waveform\", \"location\": \"https://happy-research.s3.ap-southeast-1.amazonaws.com/waveform.csv\" }\n",
    "]\n",
    "\n",
    "scores = [[\"Dataset\", \"Relative Frequency\", \"Semantic Variation\"]]\n",
    "\n",
    "for o in datasets:\n",
    "    # Instantiate pandas DataFrame\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Chunk size for reading data\n",
    "    chunksize = 10000\n",
    "\n",
    "    # The reference to the dataset. Change this to \n",
    "    dataset_file = o[\"location\"]\n",
    "\n",
    "    print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "    # Read each chunk and append to data frame\n",
    "    for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "        print(\"Reading chunk %d\" % (i + 1))\n",
    "        data = data.append(chunk)\n",
    "\n",
    "    print(\"Done loading dataset %s...\" % (o[\"name\"]))\n",
    "    \n",
    "    # Partition for percentages\n",
    "    percent_drop = 0.95\n",
    "    input_dim = len(data.columns) - 1\n",
    "    df_subset = data[data[input_dim] == -1].sample(frac=percent_drop)\n",
    "    data = data.drop(df_subset.index)\n",
    "    \n",
    "    score_rf = score_relative_frequency(data)\n",
    "    score_sv = score_semantic_variation(data)\n",
    "    scores.append([\n",
    "        o[\"name\"],\n",
    "        score_rf,\n",
    "        score_sv\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a39340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result in tabular format\n",
    "tabulate.tabulate(scores, tablefmt='html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
