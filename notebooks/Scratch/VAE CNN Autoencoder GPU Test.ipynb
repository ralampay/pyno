{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b86c99",
   "metadata": {},
   "source": [
    "# VAE CNN Autoencoder GPU Test\n",
    "\n",
    "Notebook for testing class `VaeCnnAutoencoder` features namely:\n",
    "\n",
    "1. Initialization with parameters\n",
    "2. Training with `fit` method using the GPU\n",
    "\n",
    "In this example, we will be loading images from `input_img_dir` for testing. Images will be 50 width and 50 height in dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and path relative to project\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../pyno/lib'))\n",
    "\n",
    "from vae_cnn_autoencoder import VaeCnnAutoencoder\n",
    "from abstract_dataset import AbstractDataset\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"CUDA not available\")\n",
    "    \n",
    "# CUDA related information\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION', )\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0116d",
   "metadata": {},
   "source": [
    "## Initialization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_dir = \"/home/ralampay/Pictures/niece\"\n",
    "output_model_file = \"/home/ralampay/Desktop/cnn-model.pth\"\n",
    "channel_maps = [3, 16, 4]\n",
    "h_activation = 'relu'\n",
    "o_activation = 'sigmoid'\n",
    "device = torch.device('cuda:0')\n",
    "optimizer_type = 'adam'\n",
    "scale = 2\n",
    "padding = 1\n",
    "kernel_size = 3\n",
    "num_channels = 3\n",
    "img_width = 252\n",
    "img_height = 252\n",
    "chunk_size = 100\n",
    "\n",
    "# (Wâˆ’F+2P)/S+1 should be an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the autoencoder\n",
    "model = VaeCnnAutoencoder(\n",
    "            scale=scale,\n",
    "            channel_maps=channel_maps,\n",
    "            padding=padding,\n",
    "            kernel_size=kernel_size,\n",
    "            num_channels=num_channels,\n",
    "            img_width=img_width,\n",
    "            img_height=img_height,\n",
    "            h_activation=h_activation, \n",
    "            o_activation=o_activation, \n",
    "            device=device,  \n",
    "        )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997b453",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_tensors(input_dir, img_width, img_height):\n",
    "    images = []\n",
    "    \n",
    "    ext = ['png', 'jpg', 'gif']    # Add image formats here\n",
    "\n",
    "    files = []\n",
    "    [files.extend(glob.glob(input_img_dir + '/*.' + e)) for e in ext]\n",
    "    \n",
    "    dim = (img_width, img_height)\n",
    "    \n",
    "    images = np.array([cv2.resize(cv2.imread(file), dim) for file in files])\n",
    "    \n",
    "    images = images / 255\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for img in images:\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#         print(img.shape)\n",
    "        result = img.transpose((2, 0, 1))\n",
    "        x.append(result)\n",
    "    \n",
    "    return torch.tensor(x).float()\n",
    "\n",
    "x = load_image_tensors(input_img_dir, img_width, img_height)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e3a91",
   "metadata": {},
   "source": [
    "## Training with `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db528814",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "optimizer_type = \"adam\"\n",
    "\n",
    "# Reset errors to empty list\n",
    "model.errs = []\n",
    "\n",
    "data        = AbstractDataset(x)\n",
    "dataloader  = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "if optimizer_type == \"adam\":\n",
    "    model.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "else:\n",
    "    raise Exception(\"Invalid optimizer_type: {}\".format(optimizer_type))\n",
    "\n",
    "num_iterations = len(x) / batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    curr_loss = 0 \n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        output, mu, log_var = model.forward(inputs)\n",
    "        \n",
    "         # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "        kl_divergence = 0.5 * torch.sum(-1 - log_var + mu.pow(2) + log_var.exp())\n",
    "        loss = F.binary_cross_entropy(output, inputs, size_average=False) + kl_divergence\n",
    "        \n",
    "#         print(output.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        loss = model.criterion(output, labels)\n",
    "\n",
    "        curr_loss += loss\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        # display the images\n",
    "        raw_output = output.detach().cpu().numpy()\n",
    "        \n",
    "#         for o in raw_output:\n",
    "#             result = o.transpose((1, 2, 0))\n",
    "#             plt.imshow(result)\n",
    "#             plt.show()\n",
    "\n",
    "    curr_loss = curr_loss / num_iterations\n",
    "\n",
    "    print(\"Epoch: %i\\tLoss: %0.5f\" % (epoch + 1, curr_loss.item()))\n",
    "\n",
    "    model.errs.append(curr_loss.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998b010",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da71465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = model.forward(x).detach().cpu().numpy()\n",
    "# print(y.shape)\n",
    "\n",
    "# for img in y:\n",
    "#     result = img.transpose((1, 2, 0))\n",
    "#     print(result)\n",
    "#     plt.imshow(result)\n",
    "#     plt.show()\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create noise\n",
    "noise = torch.randn(100, model.z_dim).to(device)\n",
    "y = model.deconv(noise).detach().cpu().numpy()\n",
    "dir_output_images = \"/home/ralampay/Pictures/niece-random-vae\"\n",
    "counter = 0\n",
    "for img in y:\n",
    "    result = img.transpose((1, 2, 0))\n",
    "    result = result * 255\n",
    "    file_to_write = \"{}/{}.jpg\".format(dir_output_images, str(counter))\n",
    "    cv2.imwrite(file_to_write, result)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d45c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a02e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
