{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b86c99",
   "metadata": {},
   "source": [
    "# Autoencoder Test\n",
    "\n",
    "Notebook for testing class `Autoencoder` features namely:\n",
    "\n",
    "1. Initialization with parameters\n",
    "2. Training with `fit` method\n",
    "3. Dimensionality reduction with `encode`\n",
    "\n",
    "In this example, we will be loading the `creditcardfraud.csv` dataset for testing. The dataset has `29` dimensions. The topology of the model will be `29` for the input layer, `27` for the first hidden layer and `25` for the innermost hidden layer (also known as the latent layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961a111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and path relative to project\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../pyno/lib'))\n",
    "\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0116d",
   "metadata": {},
   "source": [
    "## Initialization Parameters\n",
    "\n",
    "### `layers`\n",
    "\n",
    "An array of integers corresponding to the neuron count from input to innermost latent layer of the autoencoder.\n",
    "\n",
    "### `h_activation`\n",
    "\n",
    "The activation function to be used for hidden layers. Possible values:\n",
    "\n",
    "* `relu`\n",
    "\n",
    "Default value: `relu`\n",
    "\n",
    "### `o_activation`\n",
    "\n",
    "The activation function to be used for the output layer. Possible values:\n",
    "\n",
    "* `sigmoid`\n",
    "\n",
    "Default value: `sigmoid`\n",
    "\n",
    "### `device`\n",
    "\n",
    "Torch device to be used for training. Default is `torch.device(\"cpu\")` (use CPU of machine)\n",
    "\n",
    "### `error_type`\n",
    "\n",
    "The algorithm to be used when computing the overall error for an epoch as well as the `diff` function to determine the residual error between input and output. Possible values:\n",
    "\n",
    "* `mse`\n",
    "\n",
    "Default value: `mse`\n",
    "\n",
    "### `optimizer_type`\n",
    "\n",
    "The torch optimizer to be used for back propagation. Possible values:\n",
    "\n",
    "* `adam`\n",
    "\n",
    "Default value: `adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdfd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topology of the model from input layer to innermost latent layer\n",
    "layers = [29, 27, 25]\n",
    "\n",
    "h_activation = 'relu'\n",
    "o_activation = 'sigmoid'\n",
    "device = torch.device('cpu')\n",
    "error_type = 'mse'\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Initialize the autoencoder\n",
    "autoencoder = Autoencoder(\n",
    "                layers=layers, \n",
    "                h_activation=h_activation, \n",
    "                o_activation=o_activation, \n",
    "                device=device, \n",
    "                error_type=error_type, \n",
    "                optimizer_type=optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997b453",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Loading the dataset involves loading a file to a `pandas` `DataFrame` instance. The dataset to be loaded should be in the form of a CSV file without any headers. To avoid consuming too much memory from a single read for large datasets, provide a `chunk_size` value (integer) to determine how much rows will be loaded to the `DataFrame` per read. The format should be as follows:\n",
    "\n",
    "```\n",
    "x1,x2,x3...xn,y\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "`x1,x2,x3...xn` is the multivariate dataset and `y` is the label for that dataset. `y = 1` if the row corresponds to normal data while `y = -1` if the row corresponds to an anomaly.\n",
    "\n",
    "### Data Segregation\n",
    "\n",
    "Data will be stripped off the last column to retain only the multivariate data. It will be separated into the variables `positive_data` for all normal data and `negative_data` for all anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76af2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset '../data/creditcardfraud.csv'...\n",
      "Reading chunk 1\n",
      "Reading chunk 2\n",
      "Reading chunk 3\n",
      "Reading chunk 4\n",
      "Reading chunk 5\n",
      "Reading chunk 6\n",
      "Reading chunk 7\n",
      "Reading chunk 8\n",
      "Reading chunk 9\n",
      "Reading chunk 10\n",
      "Reading chunk 11\n",
      "Reading chunk 12\n",
      "Reading chunk 13\n",
      "Reading chunk 14\n",
      "Reading chunk 15\n",
      "Reading chunk 16\n",
      "Reading chunk 17\n",
      "Reading chunk 18\n",
      "Reading chunk 19\n",
      "Reading chunk 20\n",
      "Reading chunk 21\n",
      "Reading chunk 22\n",
      "Reading chunk 23\n",
      "Reading chunk 24\n",
      "Reading chunk 25\n",
      "Reading chunk 26\n",
      "Reading chunk 27\n",
      "Reading chunk 28\n",
      "Reading chunk 29\n",
      "Done loading dataset...\n",
      "Input Dimensionality: 29\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pandas DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Chunk size for reading data\n",
    "chunksize = 10000\n",
    "\n",
    "# The reference to the dataset. Change this to \n",
    "dataset_file = '../data/creditcardfraud.csv'\n",
    "\n",
    "print(\"Loading dataset '{}'...\".format(dataset_file))\n",
    "\n",
    "# Read each chunk and append to data frame\n",
    "for i, chunk in enumerate(pd.read_csv(dataset_file, header=None, chunksize=chunksize)):\n",
    "    print(\"Reading chunk %d\" % (i + 1))\n",
    "    data = data.append(chunk)\n",
    "\n",
    "print(\"Done loading dataset...\")\n",
    "    \n",
    "# Check for proper value of input dimensionality to be used by model\n",
    "input_dim = len(data.columns) - 1\n",
    "print(\"Input Dimensionality: %d\" % (input_dim))\n",
    "\n",
    "# Partition the data into positive_data and negative_data\n",
    "positive_data = data[data[input_dim] == 1].iloc[:,:input_dim]\n",
    "negative_data = data[data[input_dim] == -1].iloc[:,:input_dim]\n",
    "\n",
    "# x representing all data regardless of label.\n",
    "# Need to convert it to a tensor before passing it to the model for training\n",
    "x = torch.tensor(data.iloc[:,:input_dim].values).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e3a91",
   "metadata": {},
   "source": [
    "## Training with `fit` method\n",
    "\n",
    "The `fit` method in the `Autoencoder` class takes in the dataset for training using back propogation. The parameters are as follows:\n",
    "\n",
    "### `x`\n",
    "\n",
    "The vector of multivariate data representing the training set. No labels should be included. This will also be the value of `y` in terms of autoencoder training.\n",
    "\n",
    "### `epochs`\n",
    "\n",
    "Number of iterations for training the entire dataset. Default value is `100`\n",
    "\n",
    "###  `lr`\n",
    "\n",
    "The learning rate for back propagtion. Values should be `> 0` and `< 1`. Default value is `0.005`.\n",
    "\n",
    "### `batch_size`\n",
    "\n",
    "Number of records to be included in a batch for mini-batch training. Default value is `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db528814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ralampay/workspace/pyno/env/lib/python3.8/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 0.68170\n",
      "Epoch: 2\tLoss: 0.22920\n",
      "Epoch: 3\tLoss: 0.21586\n",
      "Epoch: 4\tLoss: 0.21478\n",
      "Epoch: 5\tLoss: 0.21440\n",
      "Epoch: 6\tLoss: 0.21405\n",
      "Epoch: 7\tLoss: 0.21366\n",
      "Epoch: 8\tLoss: 0.21322\n",
      "Epoch: 9\tLoss: 0.21240\n",
      "Epoch: 10\tLoss: 0.21094\n",
      "Epoch: 11\tLoss: 0.20734\n",
      "Epoch: 12\tLoss: 0.20266\n",
      "Epoch: 13\tLoss: 0.19867\n",
      "Epoch: 14\tLoss: 0.19734\n",
      "Epoch: 15\tLoss: 0.19683\n",
      "Epoch: 16\tLoss: 0.19628\n",
      "Epoch: 17\tLoss: 0.19557\n",
      "Epoch: 18\tLoss: 0.19450\n",
      "Epoch: 19\tLoss: 0.19266\n",
      "Epoch: 20\tLoss: 0.18931\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.005\n",
    "batch_size = 10000\n",
    "\n",
    "autoencoder.fit(\n",
    "    x, \n",
    "    epochs=epochs, \n",
    "    lr=lr,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e098d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
